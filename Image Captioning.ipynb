{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image Captioning.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/22_Image_Captioning.ipynb","timestamp":1559280641274}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yFG38jJESU8a","colab_type":"text"},"source":["## Imports & Settings "]},{"cell_type":"code","metadata":{"id":"YgxQvr2LSU8a","colab_type":"code","outputId":"8de3ed74-6f09-46db-8204-ebf68d4e6491","executionInfo":{"status":"ok","timestamp":1559316922413,"user_tz":-540,"elapsed":58263,"user":{"displayName":"문현주","photoUrl":"","userId":"10869622948243955466"}},"colab":{"base_uri":"https://localhost:8080/","height":142}},"source":["%matplotlib inline\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","import sys\n","\n","sys.path.insert(0, '/content/gdrive/My Drive/Colab Notebooks/2019-1_ImageCaptioning_Proj')\n","\n","print(sys.path)\n","\n","from cache import cache\n","import coco\n","\n","\n","import tensorflow as tf\n","import numpy as np\n","import sys\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras.models import Model\n","from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n","\n","from tensorflow.python.keras.optimizers import RMSprop\n","from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n","from tensorflow.python.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.preprocessing.sequence import pad_sequences"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","['/content/gdrive/My Drive/Colab Notebooks/2019-1_ImageCaptioning_Proj', '', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"52QTECRzSU8n","colab_type":"text"},"source":["##  Data Loading\n","\n","\n","http://cocodataset.org"]},{"cell_type":"code","metadata":{"id":"7ooHZGTISU8p","colab_type":"code","outputId":"78366919-538e-48fd-92c7-eeb0efb8e597","executionInfo":{"status":"ok","timestamp":1559307839187,"user_tz":-540,"elapsed":2670,"user":{"displayName":"문현주","photoUrl":"","userId":"10869622948243955466"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["\n","\n","coco.maybe_download_and_extract()\n","\n","_, filenames_train, captions_train = coco.load_records(train=True)\n","\n","\n","_, filenames_val, captions_val = coco.load_records(train=False)\n","\n","\n","num_images_train = len(filenames_train)\n","num_images_train\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading http://images.cocodataset.org/zips/train2017.zip\n","- Download progress: 16.5%"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wP2r2X9PSU8_","colab_type":"text"},"source":["### Helper-Functions for Loading and Showing Images\n","\n","This is a helper-function for loading and resizing an image."]},{"cell_type":"code","metadata":{"id":"IxTg1g6aSU9A","colab_type":"code","colab":{}},"source":["def load_image(path, size=None):\n","\n","    img = Image.open(path)\n","    if not size is None:\n","        img = img.resize(size=size, resample=Image.LANCZOS)\n","\n","    img = np.array(img)\n","\n","    img = img / 255.0\n","\n","    if (len(img.shape) == 2):\n","        img = np.repeat(img[:, :, np.newaxis], 3, axis=2)\n","    return img\n","  \n","   \n","def show_image(idx, train):\n","    \"\"\"\n","    Load and plot an image from the training- or validation-set\n","    with the given index.\n","    \"\"\"\n","\n","    if train:\n","        dir = coco.train_dir\n","        filename = filenames_train[idx]\n","        captions = captions_train[idx]\n","    else:\n","        dir = coco.val_dir\n","        filename = filenames_val[idx]\n","        captions = captions_val[idx]\n","\n","    path = os.path.join(dir, filename)\n","\n","    for caption in captions:\n","        print(caption)\n","    img = load_image(path)\n","    plt.imshow(img)\n","    plt.show()\n","    \n","show_image(idx=1, train=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8IjCdow-SU9Q","colab_type":"text"},"source":["\n","\n","## Pre-Trained Image Model 비교\n","\n"]},{"cell_type":"code","metadata":{"id":"z57Y9dHOSU9R","colab_type":"code","colab":{}},"source":["\n","from tensorflow.python.keras.applications import VGG16\n","from tensorflow.python.keras.applications import InceptionV3\n","from tensorflow.python.keras.applications.resnet50  import ResNet50\n","\n","\n","#image_model = VGG16(include_top=True, weights='imagenet')\n","\n","image_model = InceptionV3(include_top=True, weights='imagenet')\n","\n","#image_model = VGG16(include_top=True, weights='ResNet152V2')\n","\n","\n","\n","image_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sNFWdmXSU9a","colab_type":"code","colab":{}},"source":["# vgg 16일 때\n","#transfer_layer = image_model.get_layer('fc2')\n","\n","# Inception V3일 때 \n","transfer_layer = image_model.get_layer('avg_pool')\n","\n","image_model_transfer = Model(inputs=image_model.input,\n","                             outputs=transfer_layer.output)\n","\n","img_size = K.int_shape(image_model.input)[1:3]\n","print(img_size)\n","\n","transfer_values_size = K.int_shape(transfer_layer.output)[1]\n","print(transfer_values_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xPCdrTBSU9e","colab_type":"text"},"source":["The model expects input images to be of this size:"]},{"cell_type":"markdown","metadata":{"id":"zGAHS5jzSU9j","colab_type":"text"},"source":["### Process All Images\n","\n"]},{"cell_type":"code","metadata":{"id":"nJry_VHpSU9k","colab_type":"code","colab":{}},"source":["def print_progress(count, max_count):\n","\n","    pct_complete = count / max_count\n","\n","    msg = \"\\r- Progress: {0:.1%}\".format(pct_complete)\n","\n","    sys.stdout.write(msg)\n","    sys.stdout.flush()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yn-xny8gSU9m","colab_type":"text"},"source":["This is the function for processing the given files using the VGG16-model and returning their transfer-values."]},{"cell_type":"code","metadata":{"id":"Zi1El6SlSU9n","colab_type":"code","colab":{}},"source":["def process_images(data_dir, filenames, batch_size=32):\n","\n","    num_images = len(filenames)\n","\n","    shape = (batch_size,) + img_size + (3,)\n","    image_batch = np.zeros(shape=shape, dtype=np.float16)\n","\n","    shape = (num_images, transfer_values_size)\n","    transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","\n","    start_index = 0\n","\n","    while start_index < num_images:\n","        print_progress(count=start_index, max_count=num_images)\n","\n","        end_index = start_index + batch_size\n","\n","        if end_index > num_images:\n","            end_index = num_images\n","\n","        current_batch_size = end_index - start_index\n","\n","        for i, filename in enumerate(filenames[start_index:end_index]):\n","            path = os.path.join(data_dir, filename)\n","\n","            img = load_image(path, size=img_size)\n","\n","            image_batch[i] = img\n","\n","\n","        transfer_values_batch = \\\n","            image_model_transfer.predict(image_batch[0:current_batch_size])\n","\n","        transfer_values[start_index:end_index] = \\\n","            transfer_values_batch[0:current_batch_size]\n","\n","        start_index = end_index\n","\n","    print()\n","\n","    return transfer_values\n","  \n","  \n","def process_images_train():\n","    print(\"Processing {0} images in training-set ...\".format(len(filenames_train)))\n","\n","    # Path for the cache-file.\n","    cache_path = os.path.join(coco.data_dir,\n","                              \"transfer_values_train.pkl\")\n","\n","    transfer_values = cache(cache_path=cache_path,\n","                            fn=process_images,\n","                            data_dir=coco.train_dir,\n","                            filenames=filenames_train)\n","\n","    return transfer_values\n","  \n","def process_images_val():\n","    print(\"Processing {0} images in validation-set ...\".format(len(filenames_val)))\n","\n","    # Path for the cache-file.\n","    cache_path = os.path.join(coco.data_dir, \"transfer_values_val.pkl\")\n","\n","    transfer_values = cache(cache_path=cache_path,\n","                            fn=process_images,\n","                            data_dir=coco.val_dir,\n","                            filenames=filenames_val)\n","\n","    return transfer_values\n","  \n","  \n","\n","transfer_values_train = process_images_train()\n","print(\"dtype:\", transfer_values_train.dtype)\n","print(\"shape:\", transfer_values_train.shape)\n","\n","\n","\n","transfer_values_val = process_images_val()\n","print(\"dtype:\", transfer_values_val.dtype)\n","print(\"shape:\", transfer_values_val.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MKT24M0LSU93","colab_type":"text"},"source":["## Tokenizer\n"]},{"cell_type":"code","metadata":{"id":"bBSekOVXSU96","colab_type":"code","colab":{}},"source":["\n","mark_start = 'start '\n","mark_end = ' end'\n","\n","\n","def mark_captions(captions_listlist):\n","    captions_marked = [[mark_start + caption + mark_end\n","                        for caption in captions_list]\n","                        for captions_list in captions_listlist]\n","    \n","    return captions_marked\n","  \n","  \n","captions_train_marked = mark_captions(captions_train)\n","captions_train_marked[0]\n","\n","\n","print(captions_train[0])\n","\n","\n","def flatten(captions_listlist):\n","    captions_list = [caption\n","                     for captions_list in captions_listlist\n","                     for caption in captions_list]\n","    \n","    return captions_list\n","  \n","  \n","captions_train_flat = flatten(captions_train_marked)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XcjTDaxOSU-V","colab_type":"code","colab":{}},"source":["num_words = 10000\n","\n","\n","class TokenizerWrap(Tokenizer):\n","    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n","    \n","    def __init__(self, texts, num_words=None):\n","\n","        Tokenizer.__init__(self, num_words=num_words)\n","\n","        self.fit_on_texts(texts)\n","        # Create inverse lookup from integer-tokens to words.\n","        self.index_to_word = dict(zip(self.word_index.values(),\n","                                      self.word_index.keys()))\n","\n","    def token_to_word(self, token):\n","        \"\"\"Lookup a single word from an integer-token.\"\"\"\n","\n","        word = \" \" if token == 0 else self.index_to_word[token]\n","        return word \n","\n","    def tokens_to_string(self, tokens):\n","        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n","\n","        # Create a list of the individual words.\n","        words = [self.index_to_word[token]\n","                 for token in tokens\n","                 if token != 0]\n","        \n","\n","        text = \" \".join(words)\n","        return text\n","    \n","    def captions_to_tokens(self, captions_listlist):\n","       \n","        tokens = [self.texts_to_sequences(captions_list)\n","                  for captions_list in captions_listlist]\n","        return tokens\n","      \n","      \n","\n","tokenizer = TokenizerWrap(texts=captions_train_flat,\n","                          num_words=num_words)\n","\n","token_start = tokenizer.word_index[mark_start.strip()]\n","token_end = tokenizer.word_index[mark_end.strip()]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OW3Wd-pYSU-9","colab_type":"text"},"source":["## Data Generator\n"]},{"cell_type":"code","metadata":{"id":"Pd3xKmZBSU-9","colab_type":"code","colab":{}},"source":["\n","tokens_train = tokenizer.captions_to_tokens(captions_train_marked)\n","\n","def get_random_caption_tokens(idx):\n","\n","    \n","    # Initialize an empty list for the results.\n","    result = []\n","\n","    # For each of the indices.\n","    for i in idx:\n","        j = np.random.choice(len(tokens_train[i]))\n","        tokens = tokens_train[i][j]\n","        result.append(tokens)\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jr-mvR2rSU_A","colab_type":"text"},"source":["This generator function creates random batches of training-data for use in training the neural network."]},{"cell_type":"code","metadata":{"id":"EIDDr8-1SU_A","colab_type":"code","colab":{}},"source":["def batch_generator(batch_size):\n","\n","    while True:\n","        # Get a list of random indices for images in the training-set.\n","        idx = np.random.randint(num_images_train,\n","                                size=batch_size)\n","\n","        transfer_values = transfer_values_train[idx]\n","        tokens = get_random_caption_tokens(idx)\n","        num_tokens = [len(t) for t in tokens]        \n","        max_tokens = np.max(num_tokens)\n","        \n","\n","        tokens_padded = pad_sequences(tokens,\n","                                      maxlen=max_tokens,\n","                                      padding='post',\n","                                      truncating='post')\n","\n","        decoder_input_data = tokens_padded[:, 0:-1]\n","        decoder_output_data = tokens_padded[:, 1:]\n","        x_data = \\\n","        {\n","            'decoder_input': decoder_input_data,\n","            'transfer_values_input': transfer_values\n","        }\n","\n","        # Dict for the output-data.\n","        y_data = \\\n","        {\n","            'decoder_output': decoder_output_data\n","        }\n","        \n","        yield (x_data, y_data)\n","        \n","batch_size = 512\n","\n","generator = batch_generator(batch_size=batch_size)\n","\n","batch = next(generator)\n","batch_x = batch[0]\n","batch_y = batch[1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0KPpT0L-SU_i","colab_type":"text"},"source":["### Steps Per Epoch\n"]},{"cell_type":"code","metadata":{"id":"nI-kr6_LSU_j","colab_type":"code","colab":{}},"source":["num_captions_train = [len(captions) for captions in captions_train]\n","total_num_captions_train = np.sum(num_captions_train)\n","steps_per_epoch = int(total_num_captions_train / batch_size)\n","steps_per_epoch"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LyvDBxRUSU_x","colab_type":"text"},"source":["## Create the Recurrent Neural Network\n","\n"]},{"cell_type":"code","metadata":{"id":"-ATM83Y1SU_x","colab_type":"code","colab":{}},"source":["state_size = 512\n","embedding_size = 128\n","\n","transfer_values_input = Input(shape=(transfer_values_size,),\n","                              name='transfer_values_input')\n","\n","decoder_transfer_map = Dense(state_size,\n","                             activation='tanh',\n","                             name='decoder_transfer_map')\n","\n","decoder_input = Input(shape=(None, ), name='decoder_input')\n","\n","decoder_embedding = Embedding(input_dim=num_words,\n","                              output_dim=embedding_size,\n","                              name='decoder_embedding')\n","\n","decoder_gru1 = GRU(state_size, name='decoder_gru1',\n","                   return_sequences=True)\n","decoder_gru2 = GRU(state_size, name='decoder_gru2',\n","                   return_sequences=True)\n","decoder_gru3 = GRU(state_size, name='decoder_gru3',\n","                   return_sequences=True)\n","\n","decoder_dense = Dense(num_words,\n","                      activation='linear',\n","                      name='decoder_output')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xZDjA1PbSVAy","colab_type":"text"},"source":["### Connect and Create the Training Model\n"]},{"cell_type":"code","metadata":{"id":"8jgZWqtVSVAy","colab_type":"code","colab":{}},"source":["def connect_decoder(transfer_values):\n","\n","    initial_state = decoder_transfer_map(transfer_values)\n","\n","    net = decoder_input    \n","    net = decoder_embedding(net)\n","    # Connect all the GRU layers.\n","    net = decoder_gru1(net, initial_state=initial_state)\n","    net = decoder_gru2(net, initial_state=initial_state)\n","    net = decoder_gru3(net, initial_state=initial_state)\n","\n","    \n","    decoder_output = decoder_dense(net)    \n","    return decoder_output\n","\n","  \n","decoder_output = connect_decoder(transfer_values=transfer_values_input)\n","\n","decoder_model = Model(inputs=[transfer_values_input, decoder_input],\n","                      outputs=[decoder_output])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0nEau4QSVA6","colab_type":"text"},"source":["### Loss Function\n"]},{"cell_type":"code","metadata":{"id":"iCwfN99fSVA7","colab_type":"code","colab":{}},"source":["# decoder_model.compile(optimizer=optimizer,\n","#                       loss='sparse_categorical_crossentropy')\n","\n","def sparse_cross_entropy(y_true, y_pred):\n","\n","    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n","                                                          logits=y_pred)\n","    loss_mean = tf.reduce_mean(loss)\n","    return loss_mean"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gqMgl5mvSVBJ","colab_type":"text"},"source":["### Compile the Training Model\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"9oCD4ee0SVBM","colab_type":"code","colab":{}},"source":["optimizer = RMSprop(lr=1e-3)\n","\n","decoder_target = tf.placeholder(dtype='int32', shape=(None, None))\n","\n","decoder_model.compile(optimizer=optimizer,\n","                      loss=sparse_cross_entropy,\n","                      target_tensors=[decoder_target])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUhqs_8GSVBZ","colab_type":"text"},"source":["### Callback Functions\n"]},{"cell_type":"code","metadata":{"id":"VvclqwBXSVBa","colab_type":"code","colab":{}},"source":["path_checkpoint = 'IC_checkpoint.keras'\n","callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n","                                      verbose=1,\n","                                      save_weights_only=True)\n","\n","\n","callback_tensorboard = TensorBoard(log_dir='./22_logs/',\n","                                   histogram_freq=0,\n","                                   write_graph=False)\n","\n","\n","callbacks = [callback_checkpoint, callback_tensorboard]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5N4y3ST9SVBe","colab_type":"text"},"source":["### Load Checkpoint\n"]},{"cell_type":"code","metadata":{"id":"V43uJ9pLSVBf","colab_type":"code","colab":{}},"source":["try:\n","    decoder_model.load_weights(path_checkpoint)\n","except Exception as error:\n","    print(\"Error trying to load checkpoint.\")\n","    print(error)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BATuBfWMSVBg","colab_type":"text"},"source":["### Train the Model\n"]},{"cell_type":"code","metadata":{"id":"ZOU4_lONSVBh","colab_type":"code","colab":{}},"source":["\n","decoder_model.fit_generator(generator=generator,\n","                            steps_per_epoch=steps_per_epoch,\n","                            epochs=20,\n","                            callbacks=callbacks)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lg2guydSVBi","colab_type":"text"},"source":["## Generate Captions\n","\n","This function loads an image and generates a caption using the model we have trained."]},{"cell_type":"code","metadata":{"id":"SmVi7GVCSVBj","colab_type":"code","colab":{}},"source":["def generate_caption(image_path, max_tokens=30):\n","\n","    image = load_image(image_path, size=img_size)\n","\n","    image_batch = np.expand_dims(image, axis=0)\n","\n","    transfer_values = image_model_transfer.predict(image_batch)\n","\n","    shape = (1, max_tokens)\n","    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n","    token_int = token_start\n","    output_text = ''\n","    count_tokens = 0\n","    while token_int != token_end and count_tokens < max_tokens:\n","        decoder_input_data[0, count_tokens] = token_int\n","        x_data = \\\n","        {\n","            'transfer_values_input': transfer_values,\n","            'decoder_input': decoder_input_data\n","        }\n","\n","\n","        decoder_output = decoder_model.predict(x_data)\n","        token_onehot = decoder_output[0, count_tokens, :]\n","        token_int = np.argmax(token_onehot)\n","        sampled_word = tokenizer.token_to_word(token_int)\n","        output_text += \" \" + sampled_word\n","        count_tokens += 1\n","    output_tokens = decoder_input_data[0]\n","\n","    plt.imshow(image)\n","    plt.show()\n","    \n","    print(\"Predicted caption:\")\n","    print(output_text)\n","    print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RdtlIjvhSVBl","colab_type":"text"},"source":["### Examples\n","\n","Try this with a picture of a parrot."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ghUI3-o5SVBl","colab_type":"code","colab":{}},"source":["generate_caption(\"images/parrot_cropped1.jpg\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zT1SWc7eSVBn","colab_type":"text"},"source":["Try it with a picture of a person (Elon Musk). In Tutorial #07 the Inception model mis-classified this picture as being either a sweatshirt or a cowboy boot."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"MPNZEQnJSVBn","colab_type":"code","colab":{}},"source":["generate_caption(\"images/JM.jpg\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iJkl1IiUSVBq","colab_type":"text"},"source":["Helper-function for loading an image from the COCO data-set and printing the true captions as well as the predicted caption."]},{"cell_type":"code","metadata":{"id":"I1-5zAQrSVBq","colab_type":"code","colab":{}},"source":["def generate_caption_coco(idx, train=False):\n","\n","    if train:\n","        data_dir = coco.train_dir\n","        filename = filenames_train[idx]\n","        captions = captions_train[idx]\n","    else:\n","        data_dir = coco.val_dir\n","        filename = filenames_val[idx]\n","        captions = captions_val[idx]\n","\n","    path = os.path.join(data_dir, filename)\n","\n","    generate_caption(image_path=path)\n","\n","\n","    print(\"True captions:\")\n","    for caption in captions:\n","        print(caption)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZlupeCRsSVBs","colab_type":"text"},"source":["Try this on a picture from the training-set that the model has been trained on. In some cases the generated caption is actually better than the human-generated captions."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"VFQ4gl61SVBt","colab_type":"code","colab":{}},"source":["generate_caption_coco(idx=1, train=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrlFAihiSVBv","colab_type":"text"},"source":["Here is another picture of giraffes from the training-set, so this image was also used during training of the model. But the model can't produce an accurate caption. Perhaps it needs more training, or perhaps another architecture for the Recurrent Neural Network?"]},{"cell_type":"code","metadata":{"id":"m61XXHsJSVBv","colab_type":"code","colab":{}},"source":["generate_caption_coco(idx=10, train=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hnnpop6fSVBx","colab_type":"text"},"source":["Here is a picture from the validation-set which was not used during training of the model. Sometimes the model can produce good captions for images it hasn't seen during training and sometimes it can't. Can you make a better model?"]},{"cell_type":"code","metadata":{"id":"ornh4NLoSVBx","colab_type":"code","colab":{}},"source":["generate_caption_coco(idx=1, train=False)"],"execution_count":0,"outputs":[]}]}